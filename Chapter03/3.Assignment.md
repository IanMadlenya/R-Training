# Assignment 3 for R-Training Project

With some acquaintance of the basic data structures in the R language, you can start to work with real data. This assignment aims to guide you to notice the different aspects of issues in data management and manipulation.

## Subsetting

1. On which objects one can perform `[` operation? How about `[[`? Are they functions in R? What's the difference between the two? In what cases they can be used interchangeably? In what cases they cannot be used interchangeably? In what cases they might easily be confused with one another? How to avoid these possible confusions?

2. When assignment expressions such as `x[1] <- 5` are evaluated, what function is actually called upon `x`? Answer similar questions in `1`.

3. Compare `subset` function in `base` and data-manipulation functions such as `select` and `filter` in `dplyr` in terms of their usage, syntax, performance, etc. 

4. Investigate the usage of subsetting operation on `data.table`. Compare the subsetting facilities in `data.table` with data-manipulation functions in `dplyr` in terms of their usage, syntax, performance, etc. Furthermore, compare the subsetting operations in `base`, `data.table`, `dplyr` with SQL provided by `sqldf`. What are those things that can be easily done using one package yet hard using another package?

## Missing Values

1. Do missing values usually arise in your working scenario? If so, what are the typical methods of dealing with them? What are the implicit assumptions that you actually impose when you use each of them? What are the advantages and disadvantages of each of these methods? 

2. Try all `NA`-related methods such as `na.omit` in `stats` and `na.locf` in `zoo`, and consider the cases where each method might be appropriate
 
3. Model-based methods such as `Expectation - Maximization algorithm` are sometimes powerful when appropriately used. Investigate the basic idea and methodology behind this algorithm.

4. In various cases, data-driven methods have better performance than model-based methods in dealing with missing values. Study the `Collaborative Filtering algorithm`, whose vairants are widely used in `Recommendation Systems`, and try to explain why this algorithm might lead to better recommendations for users than other algorithms that attempt to statistically model the purchasing behavior of users. 

## Data Import/Export

1. When dealing with a large `csv` file (possibly > 1GB), `read.csv` is very slow. Try `fread` in `data.table` and compare the usage and performance between the two functions. Are these functions equivalent in terms of their functionality? That is, is the functionality of `fread` a superset of `read.csv` under all circumstances? 

2. Another way of dealing with large text data is to compress it into `gz` files. Investigate the `gzfile` connection in the R language and try reading and writing compressed text data. Do the same to `RData` using `load` and `save`.

